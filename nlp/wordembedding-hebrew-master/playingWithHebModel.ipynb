{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Psycometric games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-28T00:39:16.015425",
     "start_time": "2016-10-28T00:39:15.008755"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import wordembedding\n",
    "from IPython import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-28T00:39:19.164612",
     "start_time": "2016-10-28T00:39:16.017657"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "model_w2v = wordembedding.get_word2vec_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-27T20:23:59.554305",
     "start_time": "2016-10-27T20:23:59.550949"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "one of the reasons analogies might suffer from word2vec is that there is no meaning to context and precedence but still from my few attempts it can be used as a feature / to negate possible answers <br/>\n",
    "NOTE: one can even generate future analogies using n_similirity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-28T00:49:44.225900",
     "start_time": "2016-10-28T00:49:44.212057"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "marked": true,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "לנוע:קיפאון\n",
      "\n",
      "לראות:עיוורון עם הסתברות 0.536365005286\n",
      "להשמין:דיאטה עם הסתברות 0.508220930803\n",
      "לנהוג:שכרות עם הסתברות 0.432781445023\n",
      "להתעורר:שכיבה עם הסתברות 0.649317718036\n",
      "תבנית:צלם\n",
      "\n",
      "מספר:ספרה עם הסתברות 0.34251939068\n",
      "עוף:בקר עם הסתברות 0.29899309943\n",
      "שומר:נוטר עם הסתברות 0.509229523222\n",
      "ירק:סלט עם הסתברות 0.352959134347\n",
      "סנפירים:דג\n",
      "\n",
      "רשת:דולפין עם הסתברות 0.559406738121\n",
      "מכונית:מנוע עם הסתברות 0.420432341151\n",
      "מפרש:סירה עם הסתברות 0.4814317426\n",
      "ידיים:איש עם הסתברות 0.480477694128\n"
     ]
    }
   ],
   "source": [
    "def repr_anolagy(t):\n",
    "    return \":\".join(x.encode('utf-8') for x in t)\n",
    "\n",
    "def find_analogies_probability(input_, solutions):\n",
    "    print('{}\\n'.format(repr_anolagy(input_)))\n",
    "    for solution in solutions:\n",
    "        print('{} עם הסתברות {}'.format(repr_anolagy(solution), model_w2v.n_similarity(input_, solution)))\n",
    "\n",
    "# the answer is [u'לראות',u'עיוורון']\n",
    "find_analogies_probability([u'לנוע', u'קיפאון'], [[u'לראות',u'עיוורון'],[u'להשמין',u'דיאטה'], [u'לנהוג',u'שכרות'],[u'להתעורר',u'שכיבה']])\n",
    "\n",
    "# the answer is [u'שומר',u'נוטר']\n",
    "find_analogies_probability([u'תבנית',u'צלם'], [[u'מספר',u'ספרה'],[u'עוף',u'בקר'], [u'שומר',u'נוטר'],[u'ירק',u'סלט']])\n",
    "\n",
    "# the answer is [u'מפרש',u'סירה']\n",
    "find_analogies_probability([u'סנפירים',u'דג'], [[u'רשת',u'דולפין'],[u'מכונית',u'מנוע'], [u'מפרש',u'סירה'],[u'ידיים',u'איש']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Filling Missing sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec preserves its own semantic meaning. Thus summing up all the vectors or averaging them will result in a vector which could have all the semantics preserved OR to use doc2vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
