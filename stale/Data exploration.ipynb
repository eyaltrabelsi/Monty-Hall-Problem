{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Read First"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-21T22:14:45.967011",
     "start_time": "2016-12-21T22:14:45.938804"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "####    - http://www.unofficialgoogledatascience.com/2016/10/practical-advice-for-analysis-of-large.html\n",
    "####    - https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/ \n",
    "####    - http://blog.districtdatalabs.com/data-exploration-with-python-1\n",
    "####   -  https://cdn.ampproject.org/c/s/yanirseroussi.com/2016/08/21/seven-ways-to-be-data-driven-off-a-cliff/amp/\n",
    "\n",
    "<br/><br/><img src='resources/reading materials.jpeg' width=500 align=\"left\"></img><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# High Level Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T22:39:55.840270",
     "start_time": "2016-12-20T22:39:55.835597"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### 1) Understand the domain expertise ask questions\n",
    "#### 2) Aquiring data\n",
    "#### 3)  First glance data  (can be skipped sometimes)\n",
    "#### 4) Sample if needed\n",
    "#### 5) Given a memory fitted dataframe we can future investigate\n",
    "#### 6) Consider missing values and outliers\n",
    "#### 7) Now we can do some heavy lifting exploration\n",
    "\n",
    "\n",
    "<br/><br/><img src='resources/no_details.jpg' width=520 align=\"left\"></img><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Detailed Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## 1) Understand the domain expertise ask questions\n",
    "####    - https://businessanalystlearnings.com/blog/2015/2/2/improve-your-domain-knowledge-with-this-list-of-free-courses\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-19T21:58:07.556376",
     "start_time": "2016-12-19T21:58:06.842666"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## 2) Aquiring data\n",
    "####  You can  aquire the data from following resources:\n",
    "  - fetching data using sqlalchemy from within the ipython notebook ** https://github.com/catherinedevlin/ipython-sql**\n",
    "  - fetching data using neo4j from whitin the ipython notebook  **https://github.com/versae/ipython-cypher**\n",
    "  - fetching web data using beuitifulsoup from within the ipython notebook **https://github.com/Psycojoker/ipython-beautifulsoup**\n",
    "  - Various File dumps in various formats (XML, JSON,CSV)\n",
    "  \n",
    "#### If one feel more comfortable with other file format you can use the following\n",
    "  - conversion of xml to json  using **https://github.com/Inist-CNRS/node-xml2json-command**\n",
    "  - conversion of json to csv  using **https://github.com/jehiah/json2csv**\n",
    "  - conversion of csv to json using **https://csvkit.readthedocs.io/en/0.9.1/scripts/csvjson.html**  \n",
    "  - Note when one convert xml or json to csv he need to think about the granularity of the target file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## 3) First glance data  (can be skipped sometimes)\n",
    "   #### Once you know the buisness proccess, and you have the data in your end, You should play around with your data.\n",
    "   #### For each file format there is diffent tools.\n",
    "   - for every file format try open it up in relevant editor/excel etc\n",
    "   - for JSON one can run incremental json \"queries\" using **https://pypi.python.org/pypi/jmespath-terminal/0.2.1**\n",
    "   - JSON use  , try  opening it in text editor\n",
    "   - for CSV one can run queries over csv using **https://github.com/harelba/q**, in addition one can see csv stats using  **http://csvkit.readthedocs.io/en/0.9.1/scripts/csvstat.html **\n",
    "   - Convert the data to dataframe and use excel like features on the data **https://github.com/quantopian/qgrid ** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## 3) If data doesn't fit the RAM:\n",
    "   #### Most of the faster, general tools work on ram so we should pick a sample of the data which can represent it\n",
    "   #### We use the following:\n",
    "   - Naively spliting the file to smaller files\n",
    "       - if its CSV one could using pandas's read_csv like **http://stackoverflow.com/questions/20033861/how-can-i-split-a-large-file-csv-file-7gb-in-python**\n",
    "       - if its JSON one could be using ijson like in **https://www.dataquest.io/blog/python-json-tutorial/**  \n",
    "       - if its XML one could be using **https://gist.github.com/benallard/8042835**\n",
    "   - Sampling randomly the files\n",
    "       - if its CSV there is an script **https://gist.github.com/eyaltrabelsi/ebb8da1bad2b79cf732fccb432790780**\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## 5) Given a memory fitted dataframe we can future investigate \n",
    "   #### By Far most usefull tool i know about is panda profiling **https://github.com/JosPolfliet/pandas-profiling ** it all us to know the following:\n",
    "   - Data metadata info , Number of records number of bytes\n",
    "   - Warnings regarding the data like high correlation missing values etc\n",
    "   - Identify the schema of the data aka data type and category of the variables\n",
    "   - Look at  means, median, standard deviation and histograms to understand the distribution \n",
    "   - Check Completeness , Are critical data values missing? A database with missing data values is not unusual, but when the information missing is critical, then completeness is an issue. \n",
    "   - Check Conformity, Is the data following standard data definitions? \n",
    "\t  For example, are dates in a standard format? Maintaining conformity to standard formats are important to maintaining consistent structure and nomenclature for sharing and internal data management.\n",
    "\t  Are your data values correct\n",
    "   - note: still missing Continuous Variables plotbox\n",
    "   - note: Consider practical significance, small can be sometimes usefull and big can be useless\n",
    "   \n",
    "#### Another cool tool allow one to create pivottables on dataframe using **https://github.com/nicolaskruchten/jupyter_pivottablejs ** it will allow us :\n",
    "- Slice our data\n",
    "\n",
    "    \n",
    "#### Another cool tool allow one to easily do bi variate analysis on dataframe using **https://github.com/ayush1997/visualize_ML ** it will allow us :\n",
    "- Bi-variate Analysis finds out the relationship between two variables. Here, we look for association and disassociation between variables at a pre-defined significance level. \n",
    "\n",
    "#### Another cool tool which allow visuzualization on dataframe using **https://github.com/altair-viz/altair_widgets** it will allow us :\n",
    "- Bi-variate Analysis finds out the relationship between two variables. Here, we look for association and disassociation between variables at a pre-defined significance level. \n",
    "- Small analysis\n",
    "\n",
    "#### The following issues need to be taken into considerations in this part as well\n",
    "- **Check Timeliness**, Is the data available when expected and needed?  Timeliness depends on the userâ€™s expectations and needs.** Relevant only for resources where acquiring the data is very fact**\n",
    "- **Check  Consistency** ,Does the data across several systems reflect the same information? If data is reported across multiple systems, it should have the same information.\t  \n",
    "- **Check Integrity** Is the data valid across the relationships and can all the data in a database be traced and connected? For example, in a customer database there should be a valid customer/sales relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    " ## 6) Investigate outliers  and missing values\n",
    " #### Sometime the missing values holds pattern in them, we should using the following:\n",
    " - **https://github.com/ResidentMario/missingno**\n",
    " \n",
    "</br>\n",
    "#### Sometime the outliers are actualy the most intresting part of the data thus its very important part  One can find outliers( Univariate and Multivariate)  using the following:\n",
    " - **http://stackoverflow.com/questions/23199796/detect-and-exclude-outliers-in-pandas-dataframe**\n",
    " \n",
    "    \n",
    " #### after the outliers as been found one should exlore the data like in #5       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T22:43:30.037144",
     "start_time": "2016-12-20T22:43:30.032608"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## 7) Now we can do some heavy lifting exploration:\n",
    "#### Now you suppose to understand the data and be able able to actual answer\n",
    "- The most powerfull tool here is sql, the following client allow acces to many sql dbs and to create visualization over then **https://github.com/airbnb/superset**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "<br/><br/><img src='resources/details.jpeg' width=520 align=\"left\"></img><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# **stuff to check in the future:**\n",
    "- **https://github.com/jeroenjanssens/scikit-sos**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
