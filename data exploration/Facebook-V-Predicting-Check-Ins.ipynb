{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle - Facebook recruiting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facebook has simulated a dataset, consisting of an artificial world consisting of more than 100,000 places located in a 10 km by 10 km square.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets load needed libraries and global notebook settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import scipy.signal\n",
    "import warnings\n",
    "import seaborn as sns \n",
    "from matplotlib import animation,cm\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "current_palette = sns.color_palette()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We receive 2 huge files from Facebook (test.csv, train.csv).\n",
    "We will use pd.read_csv to load data to data frame.\n",
    "And create another dataframe aggrigated using place_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/train.csv')\n",
    "df_test = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets get a feeling of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets start exploring the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets check the distribution of time column on both test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(12,3))\n",
    "\n",
    "def plot_time_hist(df,color,title,subplot):\n",
    "    counts, bins = np.histogram(df[\"time\"], bins=50)\n",
    "    binsc = bins[:-1] + np.diff(bins)/2.\n",
    "    plt.subplot(subplot)\n",
    "    plt.bar(binsc, counts/(counts.sum()*1.0), width=np.diff(bins)[0], color=color)\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Fraction\")\n",
    "    plt.title(title)\n",
    "    return counts, bins, binsc\n",
    "\n",
    "\n",
    "counts1, bins1, binsc1 = plot_time_hist(df_train, current_palette[0], 'Train', 121)\n",
    "counts2, bins2, binsc2 = plot_time_hist(df_test, current_palette[1], 'Test', 122)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Insigts:**\n",
    " \n",
    "    1. Its easy to see that the test is continue of the train dataset\n",
    "    2. Its hard to see any pattern in the time data possible reasons:\n",
    "       -  diffrent users in diffrent timezones\n",
    "       -  no pattern in the data at all\n",
    " \n",
    " \n",
    " **lets validate 1 (the test is continue of the train dataset) by ploting the train and test side by side.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(2, figsize=(12,3))\n",
    "plt.bar(binsc1, counts1/(counts1.sum()*1.0), width=np.diff(bins1)[0], color=current_palette[0], label=\"Train\")\n",
    "plt.bar(binsc2, counts2/(counts2.sum()*1.0), width=np.diff(bins2)[0], color=current_palette[1], label=\"Test\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Fraction\")\n",
    "plt.title(\"Test\")\n",
    "plt.legend() # add labels for each 'entity'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lets validate 2 (if diffrent users has diffrent timezones) by checkingh frequencies for aggregated places**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_n_place_time_dict(df,n):\n",
    "\tplaces_by_frequency = df_train.groupby('place_id')['place_id'].agg('count').sort_values(ascending=False).index.tolist()\n",
    "\tplaces_by_frequency = places_by_frequency[:n]\n",
    "\tplace_time_dict = {place_id: np.squeeze(df_train[df_train['place_id']==place_id].as_matrix(columns=['time'])) \n",
    "                   \t\tfor place_id in places_by_frequency}\n",
    "\treturn place_time_dict \n",
    "\n",
    "def get_all_autocorrs(place_time_dict,n):\n",
    "\n",
    "\t#create historgram settings (One bin per 4.0 time units)\n",
    "\thist_range = (-100000.0, 100000.0)\n",
    "\tn_bins = 50000 \n",
    "\tall_autocorrs = np.zeros((n_bins, n))\n",
    "\n",
    "\t# Get the autocorrelation between timestamps for each place\n",
    "\tplace_n = 0\n",
    "\tfor place_id,times in place_time_dict.items():\n",
    "  \t\tn_events = times.size\n",
    "  \t\tn_samples = n_events*n_events # We are still randomly choosing timestamps, but this should give good coverage\n",
    "  \t\thist_vals, bin_edges = np.histogram(np.random.choice(times, size=n_samples, replace=True) - \\\n",
    "                                    \t\t\t\t         np.random.choice(times, size=n_samples, replace=True), \n",
    "\t\t\t\t\t\t                    bins=n_bins,\n",
    "                                            range=hist_range)\n",
    "\t  \tall_autocorrs[:, place_n] = hist_vals\n",
    "  \t\tplace_n += 1\n",
    "\treturn all_autocorrs \n",
    "\n",
    "def plot_fft_of_autocorrelation(f, psd,magnitude):\n",
    "    fig, axs = plt.subplots(magnitude,1)\n",
    "\n",
    "    # Adjust the X axis to be in time points instead of 1/F\n",
    "    f /= 4.0 # Remember that there is one bin per 4.0 time units\n",
    "    f = 1.0/f # Go back to time points\n",
    " \n",
    "    for i in range(magnitude):\n",
    "        axs[i].plot(f, np.log(psd))\n",
    "        axs[i].set_title('Log FFT of autocorrelations for each place')\n",
    "        axs[i].set_xlabel('time units')\n",
    "        axs[i].set_xlim([0, 2500+97500*i])\n",
    "        axs[i].set_xticks(np.arange(0, 2500+97500*i, 100+3900*i))\n",
    "        axs[i].grid(True)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()    \n",
    "\n",
    "def investigate_fft_of_autocorrelation(df,n_most=100,magnitude=2):\n",
    "    place_time_dict = get_n_place_time_dict(df_train,n_most)\n",
    "    all_autocorrs = get_all_autocorrs(place_time_dict,n_most)\n",
    "    f, psd = scipy.signal.welch(all_autocorrs, nperseg=25000, noverlap=20000, return_onesided=True, axis=0)\n",
    "    plot_fft_of_autocorrelation(f, psd, magnitude)\n",
    "\n",
    "investigate_fft_of_autocorrelation(df_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insigts:**\n",
    " \n",
    "    1.  Its easy to see that there is correlation once we grouped by place_id which probably indicate:\n",
    "        - diffrent timezone\n",
    "        - places checkedin in diffrent times (school mostly in the morning and pubs mostly at night)\n",
    " \n",
    "    2. We can conclude the time unit from training set,We can see the peaks at 1440 ,1700 and 10000 \n",
    "       This result confirms that the time units are in minutes, because:\n",
    "       -  The peak in 1440 which is near the number of minutes in a day\n",
    "       -  The peak in 10000 which is near the number of minutes in a week\n",
    "       -  NOTE: couldnt figure why there a peak in 1700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets validate the same time unit exists in the test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "investigate_fft_of_autocorrelation(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Insigts:**\n",
    " \n",
    "    1.  Its easy to see that the same time unit (minute) is on the test set as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X Y Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_scatter_x_y(df,group_by):\n",
    "    grouped = df[group_by].value_counts().reset_index()\n",
    "    ids = grouped['index'][:10]\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(ids)))\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for id, c in zip(ids, colors):\n",
    "        x = df[df[group_by] == id]['x']\n",
    "        y = df[df[group_by] == id]['y']\n",
    "        plt.scatter(x, y, color=c)\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.xlim(-0.1,10)\n",
    "    plt.ylim(-0.1,10)\n",
    "    plt.show()\n",
    "\n",
    "plot_scatter_x_y(df_train,'place_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Insigts:**\n",
    " \n",
    "    1.  Its easy to see that there is huge variation on the x axis and small variation on the y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(12,12))\n",
    "\n",
    "def plot_accuracy_by_week(df,color,label,subplot,method):\n",
    "    df[\"week\"] = np.ceil((df[\"time\"]/(60*24*7)))\n",
    "    df_wkaccuracy = df.groupby(\"week\").agg({\"accuracy\":[np.mean, np.std]}).reset_index()\n",
    "    df_wkaccuracy.columns = [\"week\", \"mean\", \"std\"]\n",
    "    x = df_wkaccuracy[\"week\"]\n",
    "    ya = df_wkaccuracy[method]\n",
    "    plt.subplot(subplot)\n",
    "    plt.plot(x, ya, c=color, lw=3, label=label)\n",
    "    plt.legend(loc=2)\n",
    "\n",
    "plot_accuracy_by_week(df_train,current_palette[0],'Train',311,'std')\n",
    "plot_accuracy_by_week(df_test,current_palette[1],'Test',312,'std')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plot_accuracy_by_week(df_train,current_palette[0],'Train',323,'mean')\n",
    "plot_accuracy_by_week(df_test,current_palette[1],'Test',324,'mean')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " **Insigts:**\n",
    " \n",
    "    1. Seems accuracy is increasing in train with time dramaticly\n",
    "    2. Seems accuracy is decressing in test with time slowly  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
